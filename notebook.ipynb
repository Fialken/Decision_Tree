{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Second assignment: Decision trees**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trabalho realizado por:**\n",
    "\n",
    "- Constança Fernandes (up202205398)\n",
    "- Diogo Silva (up201605359)\n",
    "- João Baptista (up202207629) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Código**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estruras de dados\n",
    "\n",
    "As estruturas de dados utilizadas foram, uma **class Node**, para os nós intermediários e **LeafNode** para os nós folha e **DTree** para a árvore. \n",
    "\n",
    "Foi criada uma **class Evaluation** com funções para analisar a precisão (accuracy) da árvore, e desenhar uma matriz de confusão (confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafNode:\n",
    "    def __init__(self, classif, size, val_ori):\n",
    "        self.classif = classif      # qual a classificacao a atribuir pelo no folha\n",
    "        self.counter = size         # o numero de exemplos que originaram o no\n",
    "        self.origin_value = val_ori # valor antecedente que originou o no\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, attrib):\n",
    "        self.attribute = attrib # o attributo que selecionado para saber qual split seguir\n",
    "        self.splits = {}        # um dicionario onde as chaves sao os valores possiveis do attribute, que identificam os nos criados a partir dos mesmos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na classe “Evaluation” temos 2 funções:\n",
    "-\t**accuracy()** -> calcula a taxa de precisão 0 a 1.\n",
    "-\t**confusion_matrix()** -> imprime uma matrix de confusão com coluna para cada valor único de classificação e 2 linhas, o número de corretamente previstos e o número de incorretos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def accuracy(df_pred, df_class):\n",
    "        assert(len(df_pred) == len(df_class))\n",
    "        well_classified = 0\n",
    "        i = 0\n",
    "        for classif in df_class:\n",
    "            if df_pred[i] == classif: \n",
    "                well_classified += 1\n",
    "            i += 1\n",
    "        print(f\"Accuracy: {(well_classified/len(df_class)):.2f}\")\n",
    "\n",
    "\n",
    "    def confusion_matrix(df_pred, df_class):\n",
    "        assert(len(df_pred) == len(df_class))\n",
    "        well_classified = {}\n",
    "        wrong_classified = {}\n",
    "        #create dict for TP | FP\n",
    "        for val in list(set(df_class)):\n",
    "            assert(val in list(set(df_pred)))\n",
    "            well_classified[val] = 0\n",
    "            wrong_classified[val] = 0\n",
    "\n",
    "        i = 0\n",
    "        for classif in df_class:\n",
    "            if df_pred[i] == classif: \n",
    "                well_classified[classif] += 1\n",
    "            else:\n",
    "                wrong_classified[classif] += 1\n",
    "            i += 1\n",
    "\n",
    "        Evaluation.print_matrix(well_classified, wrong_classified)\n",
    "\n",
    "    def print_matrix(well_classified:dict, wrong_classified:dict):\n",
    "        spaces = []\n",
    "\n",
    "        res = \"Real classification was correctly predicted or wrongly\\n\"\n",
    "        #header\n",
    "        res += f\"class->\"\n",
    "        for key in well_classified.keys():\n",
    "            res += f\" | {key}\"\n",
    "            spaces.append(len(key))\n",
    "        \n",
    "        #correctly classified\n",
    "        res += \" |\\ncorrect\"\n",
    "        i = 0\n",
    "        for val in well_classified.values():\n",
    "            space = \" \"*int((spaces[i]-len(str(val)))/2)\n",
    "            res += f\" | {space}{val}{space}\" if ((spaces[i]-len(str(val))) % 2 == 0) else f\" | {space}{val}{space} \"\n",
    "            i += 1\n",
    "        \n",
    "        #wrongly classified\n",
    "        res += \" |\\nwrong  \"\n",
    "        i = 0\n",
    "        for val in wrong_classified.values():\n",
    "            space = \" \"*int((spaces[i]-len(str(val)))/2)\n",
    "            res += f\" | {space}{val}{space}\" if ((spaces[i]-len(str(val))) % 2 == 0) else f\" | {space}{val}{space} \"\n",
    "            i += 1\n",
    "        res += \" |\\n\"\n",
    "        print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares e dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def best_attribute_split(df,dy):\n",
    "    best = df.columns[0]\n",
    "    size = dy.size\n",
    "    entropy_class = entropy(dy)\n",
    "    val_gain = float('-inf')\n",
    "\n",
    "    for attri in df.columns:\n",
    "        tmp_val = 0\n",
    "\n",
    "        for val in df[attri].unique():\n",
    "            subset_dy = dy.loc[df[df[attri] == val].index]\n",
    "            tmp_val += ((len(subset_dy)/size) * entropy(subset_dy))\n",
    "\n",
    "        tmp_val = entropy_class - tmp_val\n",
    "        if val_gain < tmp_val:\n",
    "            val_gain = tmp_val\n",
    "            best = attri\n",
    "\n",
    "    return best\n",
    "\n",
    "def entropy(attribute):\n",
    "    '''calcula a entropia de cada attributo'''\n",
    "    size = attribute.size\n",
    "    ent_val = 0\n",
    "    for val in attribute.unique():\n",
    "        count = 0\n",
    "        for i in attribute.index:\n",
    "            if attribute[i] == val:\n",
    "                count += 1\n",
    "        if(count == 0): # only true when val == nan\n",
    "            count = attribute.isna().sum()\n",
    "        p = count/size\n",
    "        ent_val += (p * math.log2(p))\n",
    "    return -ent_val\n",
    "\n",
    "def all_classification_equal(setX, setY):\n",
    "    '''True if all classification are equal, False otherwise'''\n",
    "    classif = setY[setX.index[0]]\n",
    "    for i in setX.index: # get the index of line i presente in sub set (can be 1,3,..,13; not necessary linear 1..n)\n",
    "        if classif != setY[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def most_common_output(indices,setY):\n",
    "    '''usado quando o subset for vazio ou nao for possivel fazer outro split por nao haver mais attributos e a classificacao nao e unanime\n",
    "    em caso de empate entre 2 (ou mais) classificacoes e selecionada uma aleatoriamente'''\n",
    "    outputs = {}\n",
    "    for i in indices:\n",
    "        if setY[i] not in outputs.keys():\n",
    "            outputs[setY[i]] = 1\n",
    "        else: outputs[setY[i]] += 1\n",
    "   \n",
    "    max_value = max(outputs.values())\n",
    "    keys_with_max_value = [key for key, value in outputs.items() if value == max_value]\n",
    "\n",
    "    return random.choice(keys_with_max_value)\n",
    "\n",
    "def new_case(curr_node, best_node, max_counter):\n",
    "    ''' usada quando queremos prever um exemplo e nao existe um caminho direto ate a um no folha\n",
    "    entao quando chegar ao limite da arvore é calculado o output mais comum a partir da sub arvore com raiz no no atual'''\n",
    "    if type(curr_node) == LeafNode:  # If the node is a leaf\n",
    "        return curr_node\n",
    "    \n",
    "    for node in curr_node.splits.values():\n",
    "        leaf_node = new_case(node, best_node, max_counter)\n",
    "        if leaf_node.counter > max_counter:\n",
    "            best_node = leaf_node\n",
    "            max_counter = leaf_node.counter\n",
    "            \n",
    "    return best_node\n",
    "\n",
    "def print_tree(node,depth):\n",
    "    '''usada para imprimir a arvore como pedido'''\n",
    "    if type(node) == LeafNode:\n",
    "        space = \"    \"*(depth-1)\n",
    "        return f\"{space}{node.origin_value}: {node.classif} ({node.counter})\\n\"\n",
    "    \n",
    "    else:\n",
    "        space = \"    \"*depth\n",
    "        res = f\"{space}<{node.attribute}>\\n\"\n",
    "\n",
    "        for key in node.splits.keys():\n",
    "\n",
    "            if type(node.splits[key]) == Node:\n",
    "                res += f\"{space}    {key}:\\n\"\n",
    "\n",
    "            res += print_tree(node.splits[key],depth+2)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class DTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - create_DTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É a função principal para o desenvolvimento da árvore e usa as funções auxiliares: \n",
    "- **best_attribute_split()** -> vai escolher qual a coluna que cria um melhor split com base na informação ganha (information gain), método referido no livro secção 19.3.3.\n",
    "- **most_common_output()** -> retorna qual o valor que o nó folha vai classificar com base no mais comum que é atribuído ao sub set a considerar (em caso de empate escolhe aleatoriamente).\n",
    "- **all_classification_equal()** -> retorna “True” caso todos os exemplos do sub set tenham a mesma classificação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função usada para prever novos casos sem saber a sua classificação.\n",
    "- Percorre a árvore para cada exemplo e classifica.\n",
    "- Caso o exemplo percorra a árvore de uma forma que não tenha sido vista no treino, então usamos **new_case()** que retorna o nó folha com maior número de exemplos no \"counter\" a partir da sub árvore com raís em \"curr_node\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - start_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recebe o dataset completo e sem alterações.\n",
    "- Separa a coluna com as classificações (a última) do resto do dataset.\n",
    "\n",
    "- Analisa coluna a coluna do restante dataset para tratar cada tipo de dado, isto é:\n",
    "- - Caso todos os valores da sejam diferentes vai ignorar o atributo pois não tem qualquer influência.\n",
    "- - Para os atributos numéricos com mais de 10% de valores diferentes vamos discretizar com o *KBinsDiscretizer* do sklean.preprocessing agrupando os dados em 5 intervalos. (n_bins = 5)\n",
    "- - Para atributos categoricos tranforma todos os caracteres em minúsculas.\n",
    "\n",
    "Caso queiramos testar a árvore ao criar, podemos definir \"divide_df = True\" para automaticamente dividir o dataset em 70%/30% para treino/teste e visualizar a accuracy e uma matriz de confusão. (\"divide_df = False\" irá apenas criar a árvore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        self.num_nodes = 0\n",
    "\n",
    "\n",
    "    def start_algorithm(self,dfx:pd, divide_df = False):\n",
    "        dfy = dfx.iloc[:, -1]\n",
    "        dfx.drop(columns=dfy.name ,inplace=True)\n",
    "\n",
    "        '''process data based on it's category'''\n",
    "        est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "\n",
    "        for col in dfx.columns:\n",
    "            #if all values are diffent we don't need that column\n",
    "            if len(dfx) == len(dfx[col].unique()):\n",
    "                dfx.drop(columns=col ,inplace=True)\n",
    "\n",
    "            #\n",
    "            elif len(dfx[col].unique())/len(dfx) > 0.10 and (np.issubdtype(dfx[col].dtype, np.integer) or np.issubdtype(dfx[col].dtype, np.float64)):\n",
    "                dfx[col] = pd.DataFrame(est.fit_transform(dfx[col].to_frame()),columns=[col])\n",
    "\n",
    "            elif type(dfx[col].dtype) == str:\n",
    "                for i in dfx[col].index:\n",
    "                    if not pd.isna(dfx[col][i]):\n",
    "                        dfx[col][i] = dfx[col][i].lower()\n",
    "        \n",
    "\n",
    "        #70/30 to train\n",
    "        if divide_df:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(dfx, dfy, test_size=0.3)\n",
    "            self.create_DTree(X_train,y_train)\n",
    "            pred = self.predict(X_test)\n",
    "            Evaluation.accuracy(pred,y_test)\n",
    "            print()\n",
    "            Evaluation.confusion_matrix(pred,y_test)\n",
    "\n",
    "        else :\n",
    "            self.create_DTree(dfx,dfy)\n",
    "\n",
    "\n",
    "    def create_DTree(self, dx_train, dy_train, curr_node=None):\n",
    "\n",
    "        # calculate best attribute to split based in information gain and create Nodes\n",
    "        best_attrib = best_attribute_split(dx_train,dy_train)\n",
    "        #create a Node for the split\n",
    "        if curr_node==None: #only true for first iteration when root will be None\n",
    "            node = Node(best_attrib)\n",
    "            self.root = node\n",
    "        else: \n",
    "            curr_node.attribute = best_attrib\n",
    "            node = curr_node\n",
    "\n",
    "        dfa = deepcopy(dx_train)\n",
    "        dfa.drop(columns=best_attrib,inplace=True)\n",
    "\n",
    "        for val in dx_train[best_attrib].unique():\n",
    "            sub_set = dfa[dx_train[best_attrib] == val]\n",
    "\n",
    "            if len(sub_set) == 0:\n",
    "                #create a leaf node with the most common output\n",
    "                node.splits[val] = LeafNode(most_common_output(dx_train.index,dy_train),len(dx_train.index),val) #add to 'split' dictionary a leafnode for val\n",
    "                self.num_nodes += 1\n",
    "\n",
    "            elif all_classification_equal(sub_set,dy_train): \n",
    "                #create a leaf node\n",
    "                node.splits[val] = LeafNode(dy_train[sub_set.index[0]],len(sub_set.index),val) #add to 'split' dictionary a leafnode for val\n",
    "                self.num_nodes += 1\n",
    "\n",
    "            elif len(sub_set.columns) == 0:\n",
    "                #create a leaf node with the most common output\n",
    "                node.splits[val] = LeafNode(most_common_output(sub_set.index,dy_train),len(sub_set.index),val) #add to 'split' dictionary a leafnode for val\n",
    "                self.num_nodes += 1\n",
    "            \n",
    "            else:\n",
    "                #need to split again\n",
    "                node.splits[val] = Node(None)\n",
    "                self.create_DTree(sub_set, dy_train, curr_node=node.splits[val])\n",
    "                self.num_nodes += 1\n",
    "\n",
    "\n",
    "    def predict(self,df):\n",
    "        pred = []\n",
    "        for i in df.index: #each row in df to classify\n",
    "            curr_node = self.root\n",
    "            while type(curr_node) != LeafNode:\n",
    "                try:\n",
    "                    curr_node = curr_node.splits[df[curr_node.attribute][i]]\n",
    "                except:\n",
    "                    #used when a new case is seen and there is no specific branch for it\n",
    "                    #in that case we will choose the classification that occurs the most in subtree from the current node\n",
    "                    curr_node = new_case(curr_node,None, float(\"-inf\"))\n",
    "            pred.append(curr_node.classif)\n",
    "            \n",
    "        return pred\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return print_tree(self.root,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testes dos datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arvore gerada:\n",
      "<Pat>\n",
      "    Some: Yes (4)\n",
      "    Full:\n",
      "        <Hun>\n",
      "            Yes:\n",
      "                <Type>\n",
      "                    Thai:\n",
      "                        <Fri>\n",
      "                            No: No (1)\n",
      "                            Yes: Yes (1)\n",
      "                    Italian: No (1)\n",
      "                    Burger: Yes (1)\n",
      "            No: No (2)\n",
      "    nan: Yes (12)\n",
      "\n",
      "Predicts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"datasets/restaurant.csv\")\n",
    "\n",
    "arvore1 = DTree()\n",
    "arvore1.start_algorithm(df1)\n",
    "print(f\"Arvore gerada:\\n{arvore1}\\nPredicts:\")\n",
    "arvore1.predict(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O algoritmo cria uma árvore equivalente ao exemplo do livro pelo que confirmamos que está correto.\n",
    "\n",
    "- Apenas tivemos uma dificuldade ao considerar com os valores NaN (not a number) pois nesse caso o subset iria ficar vazio e em vez de calcular o mais comum com um subset, calculamos com o dataset dessa recursão e acabamos por ficamos com um valor  “count” inflacionado.\n",
    "\n",
    "- Como treinamos a árvore com todo o dataset ao fazer predict vamos ter 100% accuracy pois os dados que está a prever já foram vistos no treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arvore gerada:\n",
      "<Weather>\n",
      "    sunny:\n",
      "        <Humidity>\n",
      "            3.0: no (1)\n",
      "            4.0: no (2)\n",
      "            0.0: yes (2)\n",
      "    overcast: yes (4)\n",
      "    rainy:\n",
      "        <Windy>\n",
      "            False: yes (3)\n",
      "            True: no (2)\n",
      "\n",
      "Predicts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"datasets/weather.csv\")\n",
    "\n",
    "arvore2 = DTree()\n",
    "arvore2.start_algorithm(df2)\n",
    "print(f\"Arvore gerada:\\n{arvore2}\\nPredicts:\")\n",
    "arvore2.predict(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como no dataset restaurant, treinamos a árvore com todo o dataset e ao fazer predict vamos ter 100% accuracy pois os dados que está a prever já foram vistos no treino.\n",
    "\n",
    "Pelo output do modelo podemos concluir que as melhores condições para jogar ténis são quando está:\n",
    "- “overcast” (nublado)\n",
    "\n",
    "ou\n",
    "- “sunny” (ensolarado) e pouca humidade\n",
    "\n",
    "ou\n",
    "- “rainy” (chuvoso) e sem estar ventoso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n",
      "\n",
      "Real classification was correctly predicted or wrongly\n",
      "class-> | Iris-versicolor | Iris-setosa | Iris-virginica |\n",
      "correct |        9        |     14      |       18       |\n",
      "wrong   |        3        |      0      |       1        |\n",
      "\n",
      "Arvore gerada:\n",
      "<petalwidth>\n",
      "    4.0: Iris-virginica (14)\n",
      "    0.0: Iris-setosa (35)\n",
      "    3.0:\n",
      "        <sepalwidth>\n",
      "            2.0:\n",
      "                <petallength>\n",
      "                    3.0:\n",
      "                        <sepallength>\n",
      "                            3.0: Iris-virginica (3)\n",
      "                            2.0: Iris-virginica (4)\n",
      "                    2.0: Iris-versicolor (1)\n",
      "                    4.0: Iris-virginica (1)\n",
      "            1.0: Iris-virginica (8)\n",
      "    1.0:\n",
      "        <sepalwidth>\n",
      "            1.0: Iris-versicolor (1)\n",
      "            0.0: Iris-versicolor (4)\n",
      "            3.0: Iris-setosa (1)\n",
      "    2.0:\n",
      "        <petallength>\n",
      "            2.0: Iris-versicolor (22)\n",
      "            3.0:\n",
      "                <sepalwidth>\n",
      "                    2.0: Iris-versicolor (4)\n",
      "                    0.0: Iris-virginica (1)\n",
      "                    1.0:\n",
      "                        <sepallength>\n",
      "                            2.0: Iris-versicolor (3)\n",
      "                            3.0: Iris-versicolor (2)\n",
      "            1.0: Iris-versicolor (1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv(\"datasets/iris.csv\")\n",
    "\n",
    "arvore3 = DTree()\n",
    "arvore3.start_algorithm(df3,divide_df=True) \n",
    "# divide_df = True significa que ao desenvolver o algoritmo vamos dividir o dataset em 70% treino e 30% teste\n",
    "# ao fazer divide_df=True o algoritmo vai fazer predict aos 30% de teste\n",
    "# e calcular a accuracy e criar uma matrix de confusao para a classificacao real (se foi bem prevista ou nao)\n",
    "print(f\"Arvore gerada:\\n{arvore3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo apresenta uma precisão de 91% o que é uma taxa eleva, em 50 testes obtivemos uma média de 92% de acerto.\n",
    "\n",
    "Concluímos que o nosso algoritmo pode ser bem utilizado para aprender padrões e classificar casos que nunca viu anteriormente (pelo menos com este tipo de dados).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect4 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "\n",
      "Real classification was correctly predicted or wrongly\n",
      "class-> | loss | win | draw |\n",
      "correct | 3205 | 11268 | 579  |\n",
      "wrong   | 1706 | 2135  | 1374 |\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(arvore4) a arvore é extensa pelo que nao vamos imprimir'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = pd.read_csv(\"datasets/connect4.csv\")\n",
    "\n",
    "arvore4 = DTree()\n",
    "arvore4.start_algorithm(df4,divide_df=True)\n",
    "# divide_df = True significa que ao desenvolver o algoritmo vamos dividir o dataset em 70% treino e 30% teste\n",
    "# ao fazer divide_df=True o algoritmo vai fazer predict aos 30% de teste\n",
    "# e calcular a accuracy e criar uma matrix de confusao para a classificacao real (se foi bem prevista ou nao)\n",
    "'''print(arvore4) a arvore é extensa pelo que nao vamos imprimir'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de o foco deste dataset não é dividir em teste/treino testamos na mesma para analisar o algoritmo e apesar da accuracy ser menor do que no dataset Iris, com 74%\n",
    "\n",
    "Ao testar jogar 4connected A* com a heuristica do trabalho anterior vs A* com a predict da DT concluimos que usar a DT não é um bom método.\n",
    "Quando a board está praticamente vazia jogar em qualquer coluna era considerado vitória.\n",
    "\n",
    "Ao analisar o dataset de treino reparamos que cada exemplo so tinha 8 peças, entao criamos 5 boards com 7 peças para analisar as previsoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | x | - | - | - | \n",
      "| - | - | - | o | - | - | - | \n",
      "| x | - | x | o | o | - | x | \n",
      "\n",
      "Colunas que originam \"wins\": [1, 2, 3, 4, 5, 6, 7]\n",
      "Colunas que originam \"draws\": []\n",
      " Colunas que originam \"loses\": []\n",
      "\n",
      "| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | x | - | - | \n",
      "| - | - | - | - | x | - | - | \n",
      "| - | x | o | o | x | o | - | \n",
      "\n",
      "Colunas que originam \"wins\": [1, 2, 3, 4, 5, 6, 7]\n",
      "Colunas que originam \"draws\": []\n",
      " Colunas que originam \"loses\": []\n",
      "\n",
      "| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| x | - | - | - | - | - | x | \n",
      "| x | o | - | o | x | - | o | \n",
      "\n",
      "Colunas que originam \"wins\": [1, 2, 3, 4, 5, 6, 7]\n",
      "Colunas que originam \"draws\": []\n",
      " Colunas que originam \"loses\": []\n",
      "\n",
      "| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| x | - | - | - | - | - | - | \n",
      "| x | - | - | o | - | - | - | \n",
      "| o | x | - | o | x | - | - | \n",
      "\n",
      "Colunas que originam \"wins\": []\n",
      "Colunas que originam \"draws\": []\n",
      " Colunas que originam \"loses\": [1, 2, 3, 4, 5, 6, 7]\n",
      "\n",
      "| 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | - | \n",
      "| - | - | - | - | - | - | o | \n",
      "| - | - | - | - | - | x | o | \n",
      "| x | x | - | - | - | x | o | \n",
      "\n",
      "Colunas que originam \"wins\": []\n",
      "Colunas que originam \"draws\": [1, 2, 3, 4, 5, 6, 7]\n",
      " Colunas que originam \"loses\": []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Game4InLine import Game4InLine as G4Line\n",
    "import random\n",
    "game=G4Line(6,7)\n",
    "def gerar_board(game):\n",
    "    for i in range(7):\n",
    "        legal  = game.legal_moves()\n",
    "        game.play(random.choice(legal))\n",
    "\n",
    "#gerar 5 boards com 7 peças total e testar a predict da DT\n",
    "for i in range(5):\n",
    "    game=G4Line(6,7)\n",
    "    gerar_board(game)\n",
    "    print(game)\n",
    "    game.resultado_DT()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
